# evaluate.py - Documentation

## 1. Overview
`evaluate.py` is a standalone script designed to assess the performance of the Kabaddi Injury Prediction System. It compares the pipeline's output (specifically the injury risk timeline) against manually annotated ground truth labels. It computes key classification metrics like AUC, Sensitivity, Specificity, and event-level F1 scores, generating a comprehensive PDF/JSON report and visualization plots (ROC, Precision-Recall).

## 2. Technical Overview
- **Type**: Evaluation / Testing Script
- **Dependencies**: 
    - `pandas`: Data handling (Ground Truth CSV).
    - `numpy`: Numerical operations.
    - `sklearn.metrics`: Calculation of AUC, ROC, Confusion Matrix, etc.
    - `matplotlib`: Plotting ROC and PR curves.
    - `loguru`: Logging.
- **Input**: 
    - Ground Truth CSV (`frame_num`, `injury_occurred`).
    - Predictions JSON (`pipeline_summary.json` generated by `main.py`).
- **Output**: 
    - JSON Report (`evaluation_report.json`).
    - Plots (`roc_curve.png`, `precision_recall_curve.png`).
    - Console Summary.

## 3. Detailed Technical
The core is the `InjuryPredictionEvaluator` class.
- **Data Alignment**: It aligns two time-series: the ground truth (binary labels per frame) and the predicted risk scores (continuous 0-100). It creates a binary prediction by thresholding the risk score (default > 70 is High Risk).
- **Metric Calculation**: 
    - **Frame-level**: AUC, Confusion Matrix (TN, FP, FN, TP).
    - **Event-level**: Logic to group consecutive high-risk frames into "events" and compare with ground truth injury events. This provides a more realistic performance metric than frame-by-frame accuracy which can be noisy.

## 4. Workflow and Function Details

### Workflow
1.  **Load Data**: Reads Ground Truth CSV and Prediction JSON.
2.  **Align**: Matches frames present in both datasets.
3.  **Compute Metrics**: Calculates statistical metrics.
4.  **Plot**: Generates and saves evaluation curves.
5.  **Report**: Saves all results to a JSON file.

### Functions

#### `InjuryPredictionEvaluator` Class

##### `__init__(self, ground_truth_path, predictions_path)`
- **Purpose**: Constructor.
- **Details**: Loads both datasets into memory using `_load_ground_truth` and `_load_predictions`.

##### `align_data(self)`
- **Purpose**: Synchronizes the datasets.
- **Details**: Iterates through ground truth rows, looks up corresponding risk scores in predictions. Returns aligned arrays: `y_true`, `y_scores`, `y_pred` (binary).

##### `calculate_auc(self, y_true, y_scores)`
- **Purpose**: Computes ROC AUC.
- **Details**: Uses `sklearn.metrics.roc_auc_score`.

##### `calculate_event_level_accuracy(self)`
- **Purpose**: Computes Precision, Recall, F1 for *Events*.
- **Details**: Defines an event as a sequence of frames with injury/risk. Calculates overlap between predicted high-risk windows and actual injury windows.

##### `generate_report(self, output_dir)`
- **Purpose**: Main runner for evaluation.
- **Details**: Calls alignment and all calculation methods. Saves plots and JSON report.

#### `main()`
- **Purpose**: CLI entry point.
- **Details**: Uses `argparse` to handle arguments: `--ground_truth`, `--predictions`, `--output_dir`.
